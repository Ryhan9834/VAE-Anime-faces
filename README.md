# ğŸ¨ Anime Face Generation using Variational Autoencoders (VAE)

This project implements a **Variational Autoencoder (VAE)** trained on an anime face dataset. It demonstrates the core principles of probabilistic generative modeling using deep neural networks, with a focus on stable training and visual outputs.

---

## âœ¨ Key Features

- âœ… Implemented custom Encoder, Decoder, and Sampling (reparameterization trick)
- âœ… Stable KL Divergence computation with log-variance
- âœ… Binary Crossentropy-based reconstruction loss
- âœ… Visual monitoring with image grids saved every 100 steps
- âœ… Training loss curve visualization
- âœ… GIF generation from outputs to show progress

---

## ğŸ“Š Results

- Reconstruction quality improves over time
- Latent space shows smooth interpolation between generated faces
- KL loss regularization encourages meaningful compression

---

## ğŸ“ˆ Training Curve

We track and visualize loss metrics over time to ensure stable learning and convergence.

<p align="center">
  <p align="center">
    <img src="assets/loss_curve.png" width="400">
    <br>
    <i>Training Loss Curve</i>
  </p>
</p>

---

## ğŸ”„ Input vs Reconstruction

The VAE not only generates new samples but also attempts to **reconstruct input images** after encoding them into the latent space and decoding back.

This allows us to evaluate how well the model has learned to capture meaningful information from the input.

<p align="center">
  <img src="assets/reconstruction.png" width="600">
  <br>
  <i>Top: Original Images | Bottom: Reconstructed Outputs</i>
</p>

The visual similarity between inputs and reconstructions demonstrates the encoder-decoder pipeline's effectiveness.
"""

## ğŸ› ï¸ Getting Started

### 1. Clone this repo

```bash
git clone https://github.com/Ryhan9834/VAE-Anime-faces.git
cd VAE-Anime-faces
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Run the notebook

Use Jupyter or VS Code to run:

```
VAE_AnimeFace.ipynb
```

---

## ğŸŒ€ Generate Training GIF

```python
from utils import create_gif
create_gif(image_folder='generated_images', output_gif='vae_training_progress.gif', fps=5)
```

<p align="center">
  <img src="assets/vae_training_progress.gif" width="400">
  <br>
  <i>Sample Output - VAE traning progress</i>
</p>

---

## ğŸ“ Files

| File                                | Description                            |
|-------------------------------------|----------------------------------------|
| `VAE_AnimeFace.ipynb` | Full VAE implementation and training loop |
| `requirements.txt`                 | Required Python packages               |
| `anime.keras`                      | Trained model|
| `assets/`                           | store images, gif for README and documentation|
| `README.md`                        | This file|

---

## ğŸ“š References

- [Kingma & Welling, 2013 - Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)
- [Anime Face Dataset](https://github.com/bchao1/Anime-Face-Dataset)

---

## ğŸ§  Author

#### This project is developed by [Md. Ryhan Uddin](https://github.com/Ryhan9834)                
---

## â­ Show your support

If you found this helpful, give it a â­ on GitHub!